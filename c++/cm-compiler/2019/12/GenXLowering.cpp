/*
 * Copyright (c) 2019, Intel Corporation
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 */

//===----------------------------------------------------------------------===//
//
/// GenXLowering
/// ------------
///
/// GenXLowering is a function pass that lowers certain LLVM IR instructions
/// that the rest of the GenX backend cannot deal with, or to implement peephole
/// optimizations.
///
/// It also performs a few other tasks:
///
/// 1. It implements add sinking for a variable index in a region/element
///    access. This ensures that, in a sequence of operations to calculate a
///    variable index for a region/element access, any add constant is sunk to
///    the end, such that it can become a constant offset in an indirect
///    operand, and give GenXAddressCommoning more chance to common up address
///    calculations.
///
/// 2. It splits struct values where possible, by splitting all struct phi nodes
///    before running the main pass, then removing an extractvalue by using the
///    corresponding insertvalue's input instead. Any struct value used as an arg
///    or return value still remains, and needs to be dealt with by register
///    allocation.
///
/// 3. It widens some byte vector operations to short vector.
///
///    Gen has restrictions on byte operands. The jitter copes with that, but
///    sometimes it needs to do even-odd splitting, which can lead to suboptimal
///    code if cmps and predicates are involved.
///    Here we attempt to pick up the common cases by converting a byte operation
///    to short.
///    
///    Note that we might end up with the extends being baled into the instruction
///    anyway, resulting in a byte operation in vISA.
///
/// 4. Certain uses of shufflevector are lowered:
///
///    a. a splat (copy of one element across a vector);
///    b. a boolean slice (extract of a subvector) becomes rdpredregion;
///    c. a boolean unslice (insert subvector) becomes wrpredregion.
///
///    Other uses of shufflevector are not expected and cause an assertion.
///
/// 5. A Trunc is lowered to a bitcast then a region/element read with a stride.
///    GenXCoalescing will coalesce the bitcast, and possibly bale in the region
///    read, so this will hopefully save an instruction or two.
///
/// 6. Certain floating point comparison instructions are lowered.
///
/// **IR restriction**: LLVM IR instructions not supported after this pass:
///
/// * shufflevector
/// * insertelement
/// * extractelement
/// * trunc
/// * zext/sext/uitofp from (vector of) i1
/// * select on vector of i1
/// * ``llvm.uadd.with.overflow`` (the other
///   overflowing arithmetic intrinsics are not allowed by the GenX backend anyway.)
///
///
/// **IR restriction**: No simd16 gather4_typed/scatter4_typed -- they have been
/// split into 2x simd8 operations.
///
/// **IR restriction**: rdpredregion intrinsic (which is generated by this pass from
/// certain cases of shufflevector, and represents a use of part of a predicate)
/// can only be used in select, wrregion, wrpredpredregion.
///
/// **IR restriction**: wrpredregion intrinsic (which is generated by this pass from
/// certain cases of shufflevector, and represents the write of part of a predicate)
/// must have a compare as its "new value" input.
///
/// **IR restriction**: No phi node of struct type after this pass. This is only a
/// general rule; subsequent passes have been known to reintroduce them so
/// GenXLiveness has another go at splitting them up.
///
//===----------------------------------------------------------------------===//

#include "GenX.h"
#include "GenXVisa.h"
#include "GenXGotoJoin.h"
#include "GenXIntrinsics.h"
#include "GenXModule.h"
#include "GenXRegion.h"
#include "GenXSubtarget.h"
#include "llvm/ADT/PostOrderIterator.h"
#include "llvm/Analysis/CFG.h"
#include "llvm/Analysis/LoopInfo.h"
#include "llvm/IR/Constants.h"
#include "llvm/IR/DerivedTypes.h"
#include "llvm/IR/Dominators.h"
#include "llvm/IR/Function.h"
#include "llvm/IR/Instructions.h"
#include "llvm/IR/Intrinsics.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/IR/Module.h"
#include "llvm/Pass.h"
#include "llvm/Support/CommandLine.h"
#include "llvm/Support/Debug.h"
#include "llvm/Transforms/Utils/BasicBlockUtils.h"

using namespace llvm;
using namespace genx;

static cl::opt<bool> EnableGenXByteWidening("enable-genx-byte-widening", cl::init(true), cl::Hidden,
                                      cl::desc("Enable GenX byte widening."));

namespace {

// GenXLowering : legalize execution widths and GRF crossing
class GenXLowering : public FunctionPass {
  DominatorTree *DT;
  const GenXSubtarget *ST;
  SmallVector<Instruction *, 8> ToErase;
public:
  static char ID;
  explicit GenXLowering() : FunctionPass(ID), DT(nullptr) {}
  virtual StringRef getPassName() const { return "GenX lowering"; }
  void getAnalysisUsage(AnalysisUsage &AU) const;
  bool runOnFunction(Function &F);
  static bool splitStructPhi(PHINode *Phi);
private:
  bool lowerGatherScatter4Typed(CallInst *CI, unsigned IID);
  bool processInst(Instruction *Inst);
  bool lowerRdRegion(Instruction *Inst);
  bool lowerWrRegion(Instruction *Inst);
  bool lowerRdPredRegion(Instruction *Inst);
  bool lowerWrPredRegion(Instruction *Inst);
  bool lowerInsertElement(Instruction *Inst);
  bool lowerExtractElement(Instruction *Inst);
  Value *scaleInsertExtractElementIndex(Value *IdxVal, Type *ElTy, Instruction *InsertBefore);
  bool lowerTrunc(Instruction *Inst);
  bool lowerCast(Instruction *Inst);
  bool lowerSelect(SelectInst *SI);
  bool lowerBoolScalarSelect(SelectInst *SI);
  bool lowerBoolVectorSelect(SelectInst *SI);
  bool lowerBoolShuffle(ShuffleVectorInst *Inst);
  bool lowerBoolSplat(ShuffleVectorInst *SI, Value *In, unsigned Idx);
  bool lowerShuffle(ShuffleVectorInst *Inst);
  bool lowerShuffleToSelect(ShuffleVectorInst *Inst);
  bool lowerShr(Instruction *Inst);
  bool lowerExtractValue(ExtractValueInst *Inst);
  bool lowerInsertValue(InsertValueInst *Inst);
  bool lowerUAddWithOverflow(CallInst *CI);
  bool lowerFCmpInst(FCmpInst *Inst);
  bool widenByteOp(Instruction *Inst);
  bool lowerLoadStore(Instruction *Inst);
  bool lowerMul64(Instruction *Inst);
};

} // end namespace


char GenXLowering::ID = 0;
namespace llvm { void initializeGenXLoweringPass(PassRegistry &); }
INITIALIZE_PASS_BEGIN(GenXLowering, "GenXLowering", "GenXLowering", false, false)
INITIALIZE_PASS_END(GenXLowering, "GenXLowering", "GenXLowering", false, false)

FunctionPass *llvm::createGenXLoweringPass()
{
  initializeGenXLoweringPass(*PassRegistry::getPassRegistry());
  return new GenXLowering;
}

void GenXLowering::getAnalysisUsage(AnalysisUsage &AU) const
{
  AU.addPreserved<DominatorTreeWrapperPass>();
  AU.addPreserved<LoopInfoWrapperPass>();
  AU.addPreserved<GenXModule>();
}

/***********************************************************************
 * GenXLowering::runOnFunction : process one function to
 *    lower instructions as required for GenX backend.
 *
 * This does a postordered depth first traversal of the CFG,
 * processing instructions within a basic block in reverse, to
 * ensure that we see a def after its uses (ignoring phi node uses).
 * This helps peephole optimizations which generally want to be
 * approached from the top down. For example, add sinking in the index
 * of an indirect region/element wants to see the trunc before the trunc
 * is lowered to a bitcast and an element access.
 */
bool GenXLowering::runOnFunction(Function &F) {
  auto *DTWP = getAnalysisIfAvailable<DominatorTreeWrapperPass>();
  DT = DTWP ? &DTWP->getDomTree() : nullptr;
  auto P = getAnalysisIfAvailable<GenXSubtargetPass>();
  ST = P ? P->getSubtarget() : nullptr;
  // First split any phi nodes with struct type.
  splitStructPhis(&F);
  // Create a list of basic blocks in the order we want to process them, before
  // we start the lowering. This is because lowering can split a basic block.
  SmallVector<BasicBlock *, 8> BBs;
  for (auto i = po_begin(&F.getEntryBlock()), e = po_end(&F.getEntryBlock());
      i != e; ++i)
    BBs.push_back(*i);
  // Process each basic block.
  for (auto i = BBs.begin(), e = BBs.end(); i != e; ++i) {
    BasicBlock *BB = *i;
    // The effect of this loop is that we process the instructions in reverse
    // order, and we re-process anything inserted before the instruction
    // being processed.
    for (Instruction *Inst = BB->getTerminator();;) {
      processInst(Inst);
      BasicBlock *Parent = Inst->getParent();
      if (Inst != &Parent->front())
        Inst = Inst->getPrevNode();
      else {
        if (Parent == BB)
          break;
        // We have reached the start of the basic block, but it is a different
        // basic block to BB, so lowering must have split a BB. Just go back to
        // the end of the previous one.
        Inst = Parent->getPrevNode()->getTerminator();
      }
    }
  }
  // Erase the instructions that we saved in ToErase.
  for (SmallVectorImpl<Instruction *>::iterator i = ToErase.begin(), e = ToErase.end();
      i != e; ++i)
    (*i)->eraseFromParent();
  ToErase.clear();
  return true;
}

/***********************************************************************
* lowerGatherScatter4Typed : lower gather4_typed and scatter4_typed
*
* This performs two functions:
*
* 1. If the operation is simd16 rather than simd8, it splits it into two
*    simd8 instructions.
*
* 2. For a simd8 scatter/gather, when r or both v and r are zero, replace
*    with undef so that they are not encoded in the vISA instruction and the
*    message skips them.
*/
bool GenXLowering::lowerGatherScatter4Typed(CallInst *CI, unsigned IID)
{
  enum {
    MASK_IDX = 0, PRED_IDX = 1, SURF_IDX = 2,
    U_IDX = 3, V_IDX = 4, R_IDX = 5, DATA_IDX = 6
  };

  if (CI->getArgOperand(U_IDX)->getType()->getVectorNumElements() == 16) {
    // 16 wide. Split into 2 x 8 wide.
    DebugLoc DL = CI->getDebugLoc();
    unsigned NumChannels = (unsigned)cast<ConstantInt>(
      CI->getArgOperand(MASK_IDX))->getZExtValue();
    NumChannels = (NumChannels & 0xa) >> 1 | (NumChannels & 5);
    NumChannels = (NumChannels & 0xc) >> 2 | (NumChannels & 3);
    SmallVector<Value *, 8> Args[2];
    // Channel mask
    Value *V = CI->getArgOperand(MASK_IDX);
    Args[0].push_back(V);
    Args[1].push_back(V);
    // Predicate
    V = CI->getArgOperand(PRED_IDX);
    if (auto C = dyn_cast<Constant>(V)) {
      Args[0].push_back(getConstantSubvector(C, 0, 8));
      Args[1].push_back(getConstantSubvector(C, 8, 8));
    }
    else {
      Args[0].push_back(Region::createRdPredRegion(
        V, 0, 8, "typedsplit", CI, DL));
      Args[1].push_back(Region::createRdPredRegion(
        V, 8, 8, "typedsplit", CI, DL));
    }
    // surface index
    V = CI->getArgOperand(SURF_IDX);
    Args[0].push_back(V);
    Args[1].push_back(V);
    // U, V, R pixel addresses
    for (unsigned i = U_IDX; i <= R_IDX; ++i) {
      V = CI->getArgOperand(i);
      Region R(V);
      R.Width = R.NumElements = 8;
      Args[0].push_back(R.createRdRegion(V, "typedsplit", CI, DL));
      R.Offset = 32;
      Args[1].push_back(R.createRdRegion(V, "typedsplit", CI, DL));
    }
    // Data. We need to construct a new vector with 8 elements per enabled
    // color.
    V = CI->getArgOperand(DATA_IDX);
    Region RdR(V);
    RdR.Width = RdR.NumElements = 8;
    auto DataTy = VectorType::get(V->getType()->getScalarType(), 8 * NumChannels);
    Region WrR(DataTy);
    WrR.Width = WrR.NumElements = 8;
    for (unsigned Which = 0; Which != 2; ++Which) {
      Value *NewVec = UndefValue::get(DataTy);
      if (!isa<UndefValue>(V)) {
        for (unsigned Channel = 0; Channel != NumChannels; ++Channel) {
          RdR.Offset = 4 * (16 * Channel + 8 * Which);
          auto Rd = RdR.createRdRegion(V, "typedsplit", CI, DL);
          WrR.Offset = 4 * 8 * Channel;
          NewVec = WrR.createWrRegion(NewVec, Rd, "typedsplit", CI, DL);
        }
      }
      Args[Which].push_back(NewVec);
    }
    if (IID == Intrinsic::genx_gather4_typed) {
      // Create the 8 wide gather4_typed instructions.
      Instruction *Gathers[2];
      Type *Tys[] = { Args[0][DATA_IDX]->getType(),
        Args[0][PRED_IDX]->getType(), Args[0][U_IDX]->getType() };
      auto Decl = Intrinsic::getDeclaration(
        CI->getParent()->getParent()->getParent(), (Intrinsic::ID)IID, Tys);
      Gathers[0] = CallInst::Create(Decl, Args[0], CI->getName() + ".split0", CI);
      Gathers[0]->setDebugLoc(DL);
      Gathers[1] = CallInst::Create(Decl, Args[1], CI->getName() + ".split1", CI);
      Gathers[1]->setDebugLoc(DL);
      // Join the results together, starting with the old value.
      Value *NewVec = V;
      Region RdR(Gathers[0]);
      RdR.Width = RdR.NumElements = 8;
      Region WrR(NewVec);
      WrR.Width = WrR.NumElements = 8;
      for (unsigned Which = 0; Which != 2; ++Which) {
        for (unsigned Channel = 0; Channel != NumChannels; ++Channel) {
          RdR.Offset = 4 * 8 * Channel;
          auto Rd = RdR.createRdRegion(Gathers[Which], "typedjoint", CI, DL);
          WrR.Offset = 4 * (16 * Channel + 8 * Which);
          NewVec = WrR.createWrRegion(NewVec, Rd, "typedjoin", CI, DL);
        }
      }
      CI->replaceAllUsesWith(NewVec);
    }
    else {
      // Create the 8 wide scatter4_typed instructions.
      Type *Tys[] = { Args[0][PRED_IDX]->getType(), Args[0][U_IDX]->getType(),
        Args[0][DATA_IDX]->getType() };
      auto Decl = Intrinsic::getDeclaration(
        CI->getParent()->getParent()->getParent(), (Intrinsic::ID)IID, Tys);
      auto NewInst = CallInst::Create(Decl, Args[0], "", CI);
      NewInst->setDebugLoc(DL);
      NewInst = CallInst::Create(Decl, Args[1], "", CI);
      NewInst->setDebugLoc(DL);
    }
    ToErase.push_back(CI);
    return true;
  }

  // Now we must have an 8 wide scatter/gather. Do the optimization where we
  // replace trailing 0 pixel coordinates with undef.
  assert(CI->getArgOperand(U_IDX)->getType()->getVectorNumElements() == 8);
  Constant *V = dyn_cast<Constant>(CI->getArgOperand(V_IDX));
  Constant *R = dyn_cast<Constant>(CI->getArgOperand(R_IDX));

  // Only continue when R is known to be zero.
  if (R && R->isNullValue()) {
    CI->setOperand(R_IDX, UndefValue::get(R->getType()));
    if (V && V->isNullValue())
      CI->setOperand(V_IDX, UndefValue::get(V->getType()));
  }
  return false;
}


/***********************************************************************
 * processInst : process one instruction in GenXLowering
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 */
bool GenXLowering::processInst(Instruction *Inst)
{
  if (isa<InsertElementInst>(Inst))
    return lowerInsertElement(Inst);
  if (isa<ExtractElementInst>(Inst))
    return lowerExtractElement(Inst);
  if (isa<TruncInst>(Inst))
    return lowerTrunc(Inst);
  if (isa<CastInst>(Inst))
    return lowerCast(Inst);
  if (auto SI = dyn_cast<SelectInst>(Inst)) {
    if (SI->getType()->getScalarType()->isIntegerTy(1)) {
      if (SI->getType() == SI->getCondition()->getType())
        return lowerBoolVectorSelect(SI);
      return lowerBoolScalarSelect(SI);
    }
    // Try lowering a non-bool select to wrregion. If lowerSelect decides
    // not to, and it is a byte operation, widen it if necessary.
    return lowerSelect(SI) || widenByteOp(SI);
  }
  if (auto SI = dyn_cast<ShuffleVectorInst>(Inst)) {
    if (SI->getType()->getScalarType()->isIntegerTy(1))
      return lowerBoolShuffle(SI);
    return lowerShuffle(SI);
  }
  if (isa<BinaryOperator>(Inst)) {
    if (widenByteOp(Inst))
      return true;
    if (Inst->getOpcode() == Instruction::AShr
        || Inst->getOpcode() == Instruction::LShr)
      return lowerShr(Inst);
    if (Inst->getOpcode() == Instruction::Mul)
      return lowerMul64(Inst);
    return false;
  }
  if (Inst->getOpcode() == Instruction::ICmp)
    return widenByteOp(Inst);
  else if (auto CI = dyn_cast<FCmpInst>(Inst))
    return lowerFCmpInst(CI);
  if (CallInst *CI = dyn_cast<CallInst>(Inst)) {
    unsigned IntrinsicID = Intrinsic::not_intrinsic;
    if (Function *Callee = CI->getCalledFunction()) {
      IntrinsicID = Callee->getIntrinsicID();
      assert(CI->getNumArgOperands() < GenXIntrinsicInfo::OPNDMASK);
    }
    switch (IntrinsicID) {
      case Intrinsic::genx_rdregioni:
      case Intrinsic::genx_rdregionf:
        return lowerRdRegion(Inst);
      case Intrinsic::genx_wrregioni:
      case Intrinsic::genx_wrregionf:
        return lowerWrRegion(Inst);
      case Intrinsic::genx_rdpredregion:
        return lowerRdPredRegion(Inst);
      case Intrinsic::genx_wrpredregion:
        return lowerWrPredRegion(Inst);
      case Intrinsic::not_intrinsic:
        break;
      case Intrinsic::dbg_value:
      case Intrinsic::genx_absf:
      case Intrinsic::genx_absi:
        break;
      case Intrinsic::genx_gather4_typed:
      case Intrinsic::genx_scatter4_typed:
        // Special optimization, turn v = 0, r = 0 to undef, if possible.
        // Also split 16 wide -> 2x 8 wide if necessary.        
        return lowerGatherScatter4Typed(CI, IntrinsicID);
      default:
      case Intrinsic::genx_constantpred:
      case Intrinsic::genx_constanti:
      case Intrinsic::genx_constantf:
        break; // ignore
      case Intrinsic::genx_vload: {
        if (!Inst->use_empty()) {
          Value *Ptr = Inst->getOperand(0);
          LoadInst *LI = new LoadInst(Ptr, "", /*volatile*/ true, Inst);
          LI->takeName(Inst);
          LI->setDebugLoc(Inst->getDebugLoc());
          Inst->replaceAllUsesWith(LI);
        }
        ToErase.push_back(Inst);
        return true;
      }
      case Intrinsic::genx_vstore: {
        Value *Val = Inst->getOperand(0);
        Value *Ptr = Inst->getOperand(1);
        auto ST = new StoreInst(Val, Ptr, /*volatile*/true, Inst);
        ST->setDebugLoc(Inst->getDebugLoc());
        ToErase.push_back(Inst);
        return true;
      }
      case Intrinsic::uadd_with_overflow:
        return lowerUAddWithOverflow(CI);
      case Intrinsic::sadd_with_overflow:
      case Intrinsic::ssub_with_overflow:
      case Intrinsic::usub_with_overflow:
      case Intrinsic::smul_with_overflow:
      case Intrinsic::umul_with_overflow:
        Inst->getContext().emitError(
            Inst, "GenX backend cannot handle overflowing intrinsics yet");
        break;
    }
    return false;
  }
  if (ExtractValueInst *EV = dyn_cast<ExtractValueInst>(Inst))
    return lowerExtractValue(EV);
  if (InsertValueInst *IV = dyn_cast<InsertValueInst>(Inst))
    return lowerInsertValue(IV);
  if (isa<LoadInst>(Inst) || isa<StoreInst>(Inst))
    return lowerLoadStore(Inst);
  if (isa<AllocaInst>(Inst))
    Inst->getContext().emitError(Inst,
                                 "GenX backend cannot handle allocas yet");
  return false;
}

/***********************************************************************
 * lowerRdRegion : handle read region instruction
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * 1. If index is variable do add sinking on it. (This in itself does not
 *    cause this function to return true, because it does not cause the
 *    original instruction to be replaced.)
 */
bool GenXLowering::lowerRdRegion(Instruction *Inst)
{
  // Sink add in address calculation.
  Use *U = &Inst->getOperandUse(Intrinsic::GenXRegion::RdIndexOperandNum);
  *U = sinkAdd(*U);
  return false;
}

/***********************************************************************
 * lowerWrRegion : handle write region instruction
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * 1. If index is variable do add sinking on it. (This in itself does not
 *    cause this function to return true, because it does not cause the
 *    original instruction to be replaced.)
 *
 * 2. If it is a predicated byte wrregion, see if it can be widened.
 */
bool GenXLowering::lowerWrRegion(Instruction *Inst)
{
  // Sink add in address calculation.
  Use *U = &Inst->getOperandUse(Intrinsic::GenXRegion::WrIndexOperandNum);
  *U = sinkAdd(*U);
  // See if a predicated byte wrregion can be widened.
  return widenByteOp(Inst);
}

/***********************************************************************
 * lowerRdPredRegion : handle read predicate region instruction
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * rdpredregion is a GenX backend internal intrinsic, and was thus created
 * within this GenXLowering pass. However it is considered legal only if its
 * uses are all in select or wrregion or wrpredpredregion; if not we lower
 * it further here. If a use is in rdpredregion, we need to combine the two
 * rdpredregions into one.
 */
bool GenXLowering::lowerRdPredRegion(Instruction *Inst)
{
  SmallVector<CallInst *, 4> RdPredRegionUsers;
  bool Ok = true;
  for (auto ui = Inst->use_begin(), ue = Inst->use_end(); ui != ue; ++ui) {
    auto User = cast<Instruction>(ui->getUser());
    if (isa<SelectInst>(User))
      continue;
    unsigned IID = getIntrinsicID(User);
    if (isWrRegion(IID))
      continue;
    if (IID == Intrinsic::genx_wrpredpredregion)
      continue;
    if (IID == Intrinsic::genx_rdpredregion) {
      RdPredRegionUsers.push_back(cast<CallInst>(User));
      continue;
    }
    if (IID == Intrinsic::not_intrinsic) {
      Ok = false;
      break;
    }
    if (cast<CallInst>(User)->doesNotAccessMemory()) {
      Ok = false;
      break;
    }
  }
  unsigned Start = cast<ConstantInt>(Inst->getOperand(1))->getZExtValue();
  unsigned Size = Inst->getType()->getVectorNumElements();
  if (Ok) {
    // All uses in select/wrregion/rdpredregion/non-ALU intrinsic, so we can
    // keep the rdpredregion.  Check for uses in another rdpredregion; we need
    // to combine those.
    for (auto ui = RdPredRegionUsers.begin(), ue = RdPredRegionUsers.end();
        ui != ue; ++ui) {
      auto User = *ui;
      unsigned UserStart = cast<ConstantInt>(User->getOperand(1))->getZExtValue();
      unsigned UserSize = User->getType()->getVectorNumElements();
      auto Combined = Region::createRdPredRegion(Inst->getOperand(0),
          Start + UserStart, UserSize, "", User, User->getDebugLoc());
      Combined->takeName(User);
      User->replaceAllUsesWith(Combined);
      ToErase.push_back(User);
    }
    return false;
  }
  // Need to lower it further.
  DebugLoc DL = Inst->getDebugLoc();
  // Convert input to vector of short.
  auto In = Inst->getOperand(0);
  Type *I16Ty = Type::getInt16Ty(Inst->getContext());
  Type *InI16Ty = VectorType::get(I16Ty,
      In->getType()->getVectorNumElements());
  auto InI16 = CastInst::Create(Instruction::ZExt, In, InI16Ty,
      Inst->getName() + ".lower1", Inst);
  InI16->setDebugLoc(DL);
  // Use rdregion to extract the region.
  Region R(InI16);
  R.getSubregion(Start, Size);
  auto Rd = R.createRdRegion(InI16, Inst->getName() + ".lower3", Inst, DL);
  // Convert back to predicate.
  auto Res = CmpInst::Create(Instruction::ICmp, CmpInst::ICMP_NE, Rd,
      Constant::getNullValue(Rd->getType()), Inst->getName() + ".lower4", Inst);
  Res->setDebugLoc(DL);
  // Replace uses and erase.
  Inst->replaceAllUsesWith(Res);
  ToErase.push_back(Inst);
  return true;
}

/***********************************************************************
 * lowerWrPredRegion : handle write predicate region instruction
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * wrpredregion is a GenX backend internal intrinsic, and was thus created
 * within this GenXLowering pass. However it is considered legal only if its
 * "new value" input is a compare; if not we lower it further here.
 */
bool GenXLowering::lowerWrPredRegion(Instruction *Inst)
{
  auto NewVal = Inst->getOperand(1);
  if (isa<CmpInst>(NewVal))
    return false;
  // Need to lower it further.
  DebugLoc DL = Inst->getDebugLoc();
  // Convert "old value" input to vector of short.
  auto OldVal = Inst->getOperand(0);
  Type *I16Ty = Type::getInt16Ty(Inst->getContext());
  Type *OldValI16Ty = VectorType::get(I16Ty,
      OldVal->getType()->getVectorNumElements());
  auto OldValI16 = CastInst::Create(Instruction::ZExt, OldVal, OldValI16Ty,
      Inst->getName() + ".lower1", Inst);
  OldValI16->setDebugLoc(DL);
  // Convert "new value" input to vector of short.
  Type *NewValI16Ty = VectorType::get(I16Ty,
      NewVal->getType()->getVectorNumElements());
  auto NewValI16 = CastInst::Create(Instruction::ZExt, NewVal, NewValI16Ty,
      Inst->getName() + ".lower2", Inst);
  NewValI16->setDebugLoc(DL);
  // Use wrregion to write the new value into the old value.
  Region R(OldValI16);
  R.getSubregion(cast<ConstantInt>(Inst->getOperand(2))->getZExtValue(),
      NewValI16Ty->getVectorNumElements());
  auto Wr = R.createWrRegion(OldValI16, NewValI16,
      Inst->getName() + ".lower3", Inst, DL);
  // Convert back to predicate.
  auto Res = CmpInst::Create(Instruction::ICmp, CmpInst::ICMP_NE, Wr,
      Constant::getNullValue(Wr->getType()), Inst->getName() + ".lower4", Inst);
  Res->setDebugLoc(DL);
  // Replace uses and erase.
  Inst->replaceAllUsesWith(Res);
  ToErase.push_back(Inst);
  return true;
}

/***********************************************************************
 * lowerInsertElement : lower InsertElement to wrregion, multiplying the
 *      index by the element size
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 */
bool GenXLowering::lowerInsertElement(Instruction *Inst)
{
  Instruction *NewInst = NULL;
  // Special case - if the result has 1 element (usually turning scalar into 1 element vector) then
  // simply transform the insert element into a bitcast
  // We don't need to worry about the index since if it is not zero the result is undef anyway (and
  // can be set to anything we like)
  // We also don't need to worry about what the original vector is (usually undef) since it will be
  // overwritten or undef
  VectorType *VT = dyn_cast<VectorType>(Inst->getType());
  unsigned NumElements = VT->getNumElements();
  const DebugLoc &DL = Inst->getDebugLoc();
  if (NumElements == 1)
  {
    Value *ToInsert = Inst->getOperand(1);
    NewInst = CastInst::Create(Instruction::BitCast, ToInsert, VT, Inst->getName(), Inst);
    NewInst->setDebugLoc(DL);
  } else if (!Inst->getType()->getScalarType()->isIntegerTy(1)) {
    // Cast and scale the index.
    Value *IdxVal = scaleInsertExtractElementIndex(Inst->getOperand(2),
                                                   Inst->getOperand(1)->getType(), Inst);
    // Sink adds in the address calculation.
    IdxVal = sinkAdd(IdxVal);
    // Create the new wrregion
    Value *Src = Inst->getOperand(1);
    Region R(Src);
    R.Indirect = IdxVal;
    NewInst = cast<Instruction>(R.createWrRegion(Inst->getOperand(0),
        Src, Inst->getName(), Inst/*InsertBefore*/, DL));
  } else {
    // Boolean insertelement. We have to cast everything to i16, do the
    // insertelement, and cast it back again. All this gets further lowered
    // subsequently.
    auto I16Ty = Type::getIntNTy(Inst->getContext(), 16);
    auto VecTy = VectorType::get(I16Ty, Inst->getType()->getVectorNumElements());
    auto CastVec = CastInst::Create(Instruction::ZExt, Inst->getOperand(0),
        VecTy, Inst->getOperand(0)->getName() + ".casti16", Inst);
    CastVec->setDebugLoc(DL);
    auto CastEl = CastInst::Create(Instruction::ZExt, Inst->getOperand(1),
        I16Ty, Inst->getOperand(1)->getName() + ".casti16", Inst);
    CastEl->setDebugLoc(DL);
    auto NewInsert = InsertElementInst::Create(CastVec, CastEl,
        Inst->getOperand(2), "", Inst);
    NewInsert->takeName(Inst);
    NewInsert->setDebugLoc(DL);
    NewInst = CmpInst::Create(Instruction::ICmp, CmpInst::ICMP_NE, NewInsert,
        Constant::getNullValue(VecTy), NewInsert->getName() + ".casti1", Inst);
    NewInst->setDebugLoc(DL);
  }
  // Change uses and mark the old inst for erasing.
  Inst->replaceAllUsesWith(NewInst);
  ToErase.push_back(Inst);
  return true;
}

/***********************************************************************
 * lowerExtractElement : lower ExtractElement to rdregion, multiplying the
 *      index by the element size
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 */
bool GenXLowering::lowerExtractElement(Instruction *Inst)
{
  Instruction *NewInst = nullptr;
  if (!Inst->getType()->isIntegerTy(1)) {
    // Cast and scale the index.
    Type *ElTy = Inst->getType();
    Value *IdxVal = scaleInsertExtractElementIndex(Inst->getOperand(1), ElTy, Inst);
    // Sink adds in the address calculation.
    IdxVal = sinkAdd(IdxVal);
    // Create the new rdregion.
    Region R(Inst);
    R.Indirect = IdxVal;
    NewInst = R.createRdRegion(Inst->getOperand(0), Inst->getName(),
        Inst/*InsertBefore*/, Inst->getDebugLoc(), true/*AllowScalar*/);
  } else {
    // Boolean extractelement. We have to cast everything to i16, do the
    // extractelement, and cast it back again. All this gets further lowered
    // subsequently.
    auto I16Ty = Type::getIntNTy(Inst->getContext(), 16);
    auto VecTy = VectorType::get(
        I16Ty, Inst->getOperand(0)->getType()->getVectorNumElements());
    auto CastVec = CastInst::Create(Instruction::ZExt, Inst->getOperand(0),
        VecTy, Inst->getOperand(0)->getName() + ".casti16", Inst);
    DebugLoc DL = Inst->getDebugLoc();
    CastVec->setDebugLoc(DL);
    auto NewExtract = ExtractElementInst::Create(CastVec, Inst->getOperand(1),
        "", Inst);
    NewExtract->takeName(Inst);
    NewExtract->setDebugLoc(DL);
    NewInst = CmpInst::Create(Instruction::ICmp, CmpInst::ICMP_NE, NewExtract,
        Constant::getNullValue(I16Ty), NewExtract->getName() + ".casti1", Inst);
    NewInst->setDebugLoc(DL);
  }
  // Change uses and mark the old inst for erasing.
  Inst->replaceAllUsesWith(NewInst);
  ToErase.push_back(Inst);
  return true;
}

/***********************************************************************
 * scaleInsertExtractElementIndex : scale index by element byte size,
 *      and ensure it is an i16
 */
Value *GenXLowering::scaleInsertExtractElementIndex(Value *IdxVal,
    Type *ElTy, Instruction *InsertBefore)
{
  // Do the cast and multiply.
  unsigned ElementBytes = ElTy->getPrimitiveSizeInBits() / 8;
  IntegerType *I16Ty = Type::getInt16Ty(IdxVal->getContext());
  if (ConstantInt *CI = dyn_cast<ConstantInt>(IdxVal))
    return ConstantInt::get(I16Ty, CI->getSExtValue() * ElementBytes);
  // Ensure the variable offset is i16.
  Instruction *IdxInst = CastInst::CreateIntegerCast(IdxVal, I16Ty,
      false/*isSigned*/, "cast", InsertBefore);
  IdxInst->setDebugLoc(InsertBefore->getDebugLoc());
  // Multiply it by the element size in bytes.
  if (ElementBytes != 1) {
    IdxInst = BinaryOperator::Create(Instruction::Shl, IdxInst,
        ConstantInt::get(I16Ty, llvm::log2(ElementBytes)), "scale", InsertBefore);
    IdxInst->setDebugLoc(InsertBefore->getDebugLoc());
  }
  return IdxInst;
}

/***********************************************************************
 * lowerTrunc : lower a TruncInst
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * A Trunc is lowered to a bitcast then a region/element read with a stride.
 * GenXCoalescing will coalesce the bitcast, so this will hopefully save
 * an instruction.
 */
bool GenXLowering::lowerTrunc(Instruction *Inst)
{
  Value *InValue = Inst->getOperand(0);
  // Check for the trunc's input being a sext/zext where the original element
  // size is the same as the result of the trunc. We can just remove the
  // whole thing then. (This can arise from GenXReduceIntSize.)
  if (auto CI = dyn_cast<CastInst>(InValue)) {
    if ((isa<SExtInst>(CI) || isa<ZExtInst>(CI))
        && CI->getOperand(0)->getType() == Inst->getType()) {
      // Just replace uses with the original unextended value.
      Inst->replaceAllUsesWith(CI->getOperand(0));
      ToErase.push_back(Inst);
      return true;
    }
  }

  // Lower "trunc i8 %v to i1" into "cmp.ne (%v & 1), 0"
  if (Inst->getType()->isIntegerTy(1)) {
    IRBuilder<> Builder(Inst);
    auto V = Builder.CreateAnd(InValue, ConstantInt::get(InValue->getType(), 1));
    V = Builder.CreateICmpNE(V, ConstantInt::get(V->getType(), 0));
    if (auto I = dyn_cast<Instruction>(V))
      I->setDebugLoc(Inst->getDebugLoc());
    Inst->replaceAllUsesWith(V);
    ToErase.push_back(Inst);
    return true;
  }

  Type *InElementTy = InValue->getType();
  Type *OutElementTy = Inst->getType();
  unsigned NumElements = 1;
  if (VectorType *VT = dyn_cast<VectorType>(InElementTy)) {
    InElementTy = VT->getElementType();
    OutElementTy = cast<VectorType>(OutElementTy)->getElementType();
    NumElements = VT->getNumElements();
  }

  // Lower "trunc <32 x i16> %v to <32 x i1>" into "cmp.ne (%v & 1), 0"
  if (NumElements > 1 && OutElementTy->isIntegerTy(1)) {
    IRBuilder<> Builder(Inst);
    unsigned N = NumElements;
    Value *Os = ConstantVector::getSplat(N, ConstantInt::get(InElementTy, 1));
    Value *Zs = ConstantVector::getSplat(N, ConstantInt::get(InElementTy, 0));
    auto V = Builder.CreateAnd(InValue, Os);
    if (auto I = dyn_cast<Instruction>(V))
      I->setDebugLoc(Inst->getDebugLoc());
    V = Builder.CreateICmpNE(V, Zs);
    if (auto I = dyn_cast<Instruction>(V))
      I->setDebugLoc(Inst->getDebugLoc());
    Inst->replaceAllUsesWith(V);
    ToErase.push_back(Inst);
    return true;
  }

  assert(OutElementTy->getPrimitiveSizeInBits());
  unsigned Stride = InElementTy->getPrimitiveSizeInBits() / OutElementTy->getPrimitiveSizeInBits();
  // Create the new bitcast.
  Instruction *BC = CastInst::Create(Instruction::BitCast, InValue,
      VectorType::get(OutElementTy, Stride * NumElements), Inst->getName(),
      Inst/*InsertBefore*/);
  BC->setDebugLoc(Inst->getDebugLoc());
  // Create the new rdregion.
  Region R(BC);
  R.NumElements = NumElements;
  R.Stride = Stride;
  R.Width = NumElements;
  R.VStride = R.Stride * R.Width;
  Instruction *NewInst = R.createRdRegion(BC, Inst->getName(), Inst/*InsertBefore*/,
      Inst->getDebugLoc(), !isa<VectorType>(Inst->getType())/*AllowScalar*/);
  // Change uses and mark the old inst for erasing.
  Inst->replaceAllUsesWith(NewInst);
  ToErase.push_back(Inst);
  return true;
}

/***********************************************************************
 * lowerCast : lower a CastInst
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 */
bool GenXLowering::lowerCast(Instruction *Inst)
{
  // If it is zext/sext/UIToFP from (vector of) i1, turn into a select.
  if (Inst->getOperand(0)->getType()->getScalarType()->isIntegerTy(1)
      && Inst->getOpcode() != Instruction::BitCast) {
    int OneVal = 0;
    switch (Inst->getOpcode()) {
      case Instruction::ZExt: OneVal = 1; break;
      case Instruction::SExt: OneVal = -1; break;
      case Instruction::UIToFP: OneVal = 1; break;
      default:
        assert(0 && "unknown opcode in lowerCast");
    }

    Instruction *NewInst;
    if (Inst->getType()->isFloatingPointTy())
      NewInst = SelectInst::Create(Inst->getOperand(0),
          ConstantFP::get(Inst->getType(), OneVal),
          ConstantFP::get(Inst->getType(), 0), Inst->getName(), Inst);
    else
      NewInst = SelectInst::Create(Inst->getOperand(0),
          ConstantInt::get(Inst->getType(), OneVal),
          ConstantInt::get(Inst->getType(), 0), Inst->getName(), Inst);
    NewInst->setDebugLoc(Inst->getDebugLoc());
    Inst->replaceAllUsesWith(NewInst);
    ToErase.push_back(Inst);
    return true;
  }
  return false;
}

/***********************************************************************
 * lowerSelect : lower a non-i1 select
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * Normally we leave a non-boolean select as it is. However, if it is used
 * in a predicated wrregion, we convert it into a wrregion as a workaround
 * for jitterbug 3993.
 *
 * Even without the need for this workaround, we need to have a further look at
 * whether it is better to convert select to wrregion or leave it as it is. One
 * possible approach is to always convert it to wrregion, and then convert back
 * to select if it fails to get anything baled into it.
 */
bool GenXLowering::lowerSelect(SelectInst *SI)
{
  if (!isa<VectorType>(SI->getOperand(0)->getType()))
    return false; // scalar selector
  bool NeedToLower = false;
  for (auto ui = SI->use_begin(), ue = SI->use_end(); ui != ue; ++ui) {
    auto user = cast<Instruction>(SI->use_begin()->getUser());
    if (!isWrRegion(getIntrinsicID(user)))
      continue;
    Region R(user, BaleInfo());
    if (!R.Mask)
      continue;
    NeedToLower = true;
    break;
  }
  if (!NeedToLower)
    return false;
  // Convert the select to wrregion. If either input is a rdregion, prefer that
  // as the "old value" input.
  unsigned OldValInput;
  for (OldValInput = 0; OldValInput != 2; ++OldValInput) {
    auto RdRegionVal = SI->getOperand(OldValInput + 1);
    if (isWrRegion(getIntrinsicID(RdRegionVal)))
      break;
  }
  if (OldValInput == 2)
    OldValInput = 1;
  auto DL = SI->getDebugLoc();
  Value *Pred = SI->getOperand(0);
  if (isa<Constant>(Pred))
    return false; // constant predicate, don't do anything here
  if (!OldValInput) {
    // It was the "true" input that was the rdregion, rather than the "false"
    // input, so we need to invert the predicate.
    auto PredInst = BinaryOperator::Create(Instruction::Xor, Pred,
        Constant::getAllOnesValue(Pred->getType()),
        SI->getName() + ".invertpred", SI);
    PredInst->setDebugLoc(DL);
    Pred = PredInst;
  }
  Region R(SI);
  R.Mask = Pred;
  auto NewWrRegion = cast<Instruction>(R.createWrRegion(
        SI->getOperand(1 + OldValInput), SI->getOperand(2 - OldValInput),
        SI->getName() + ".lower", SI, DL));
  SI->replaceAllUsesWith(NewWrRegion);
  ToErase.push_back(SI);
  return true;
}

/***********************************************************************
 * lowerBoolScalarSelect : lower a SelectInst on vector of i1
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * This is a select on vector of i1 where the condition is scalar. This only
 * happens in simd control flow where an LLVM pass has optimized away the
 * conditional branch. We restore the conditional branch and create an
 * if..else..endif.
 */
bool GenXLowering::lowerBoolScalarSelect(SelectInst *SI)
{
  //         BB1
  //        /  |
  // false /   | true
  //      /    |
  //   BB2     |
  //      \    |
  //       \   |
  //        \  |
  //         BB4
  //
  auto BB1 = SI->getParent();
  auto BB2 = SplitBlock(BB1, SI, DT);
  auto BB4 = SplitEdge(BB1, BB2, DT);
  BB2->setName("select.false");
  BB4->setName("select.true");

  auto OldTerm = BB1->getTerminator();
  BranchInst::Create(BB4, BB2, SI->getCondition(), OldTerm);
  OldTerm->eraseFromParent();
  // Since additional edge is added between BB1 and BB4 instead of through BB2
  // only. BB4 is not immediately dominated by BB2 anymore. Instead, BB4 is
  // dominated by BB1 immediately.
  if (DT) DT->changeImmediateDominator(BB4, BB1);
  // Replace 'select' with 'phi'
  auto Phi = PHINode::Create(SI->getType(), /*NumReservedValues=*/2, "",
                             &BB4->front());
  Phi->takeName(SI);
  Phi->addIncoming(SI->getTrueValue(), BB1);
  Phi->addIncoming(SI->getFalseValue(), BB2);
  SI->replaceAllUsesWith(Phi);
  ToErase.push_back(SI);
  // Split the (critical) edge from BB1 to BB4 to avoid having critical edge.
  auto BB3 = SplitEdge(BB1, BB4, DT);
  BB3->setName("select.crit");
  return true;
}

/***********************************************************************
 * lowerBoolVectorSelect : lower a SelectInst on (vector of) i1
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * A select on (vector of) i1 is lowered to the equivalent and/or/xor
 * instructions. No simplification is done even if an input is a constant.
 *
 * However, if the selector looks like an EM value, and the "true" operand is
 * a cmp, it is instead lowered to an llvm.genx.wrpredpredregion. Baling will
 * bale the cmp into it, resulting in a masked cmp instruction that sets bits
 * of the flag only if the corresponding EM bit is set.
 *
 * FIXME: I have seen a case where the two inputs are all false and all true.
 * Rather than try and simplify that here in the GenX backend, we should
 * try and work out how to stop LLVM generating it in the first place.
 */
bool GenXLowering::lowerBoolVectorSelect(SelectInst *Inst)
{
  if (isa<CmpInst>(Inst->getTrueValue())) {
    // Check for the condition being an EM value. It might be a shufflevector
    // that slices the EM value at index 0.
    bool IsEM = GotoJoin::isEMValue(Inst->getCondition());
    if (!IsEM) {
      if (auto SV = dyn_cast<ShuffleVectorInst>(Inst->getCondition())) {
        ShuffleVectorAnalyzer SVA(SV);
        if (!SVA.getAsSlice()) {
          // Slice at index 0.
          IsEM = GotoJoin::isEMValue(SV->getOperand(0));
        }
      }
    }
    if (IsEM) {
      // Can be lowered to llvm.genx.wrpredpredregion. It always has an index of 0
      // and the "new value" operand the same vector width as the whole vector here.
      // That might get changed if it is split up in legalization.
      auto NewInst = Region::createWrPredPredRegion(Inst->getFalseValue(),
          Inst->getTrueValue(), 0, Inst->getCondition(), "", Inst,
          Inst->getDebugLoc());
      NewInst->takeName(Inst);
      Inst->replaceAllUsesWith(NewInst);
      ToErase.push_back(Inst);
      return true;
    }
  }
  // Normal lowering to some bit twiddling.
  Instruction *NewInst1 = BinaryOperator::Create(BinaryOperator::And,
      Inst->getOperand(0), Inst->getOperand(1), Inst->getName(), Inst);
  NewInst1->setDebugLoc(Inst->getDebugLoc());
  Instruction *NewInst2 = BinaryOperator::Create(BinaryOperator::Xor,
      Inst->getOperand(0), Constant::getAllOnesValue(Inst->getType()),
      Inst->getName(), Inst);
  NewInst2->setDebugLoc(Inst->getDebugLoc());
  Instruction *NewInst3 = BinaryOperator::Create(BinaryOperator::And,
      Inst->getOperand(2), NewInst2, Inst->getName(), Inst);
  NewInst3->setDebugLoc(Inst->getDebugLoc());
  Instruction *NewInst4 = BinaryOperator::Create(BinaryOperator::Or,
      NewInst1, NewInst3, Inst->getName(), Inst);
  NewInst4->setDebugLoc(Inst->getDebugLoc());
  Inst->replaceAllUsesWith(NewInst4);
  ToErase.push_back(Inst);
  return true;
}

/***********************************************************************
 * lowerBoolShuffle : lower a shufflevector (element type i1)
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * We handle three cases:
 *
 * 1. A slice of the vector, which can be turned into rdpredregion.
 *
 * 2. A splat. By default we need to lower that to a select to
 *    0 or -1 then a bitcast to the vector of i1. But if the input is the
 *    result of a cmp then we can splat the cmp as an optimization.
 *
 * 3. An unslice of the vector, which can be turned into wrpredregion.
 */
bool GenXLowering::lowerBoolShuffle(ShuffleVectorInst *SI)
{
  ShuffleVectorAnalyzer SVA(SI);
  // 1. Check for a slice.
  int SliceStart = SVA.getAsSlice();
  if (SliceStart >= 0) {
    unsigned Width = SI->getType()->getVectorNumElements();
    auto RPR = Region::createRdPredRegion(SI->getOperand(0), SliceStart, Width,
        "", SI, SI->getDebugLoc());
    RPR->takeName(SI);
    SI->replaceAllUsesWith(RPR);
    ToErase.push_back(SI);
    return true;
  }
  // 2. Check for a splat.
  auto Splat = SVA.getAsSplat();
  if (Splat.Input)
    return lowerBoolSplat(SI, Splat.Input, Splat.Index);
  // 3. Check for an unslice. The "old value" input is operand 0 of the
  // shufflevector; the "new value" input is operand 0 of the shufflevector
  // that is operand 1 of SI. We create a wrpredregion, but GenXLowering might
  // subsequently decide that it is illegal because its "new value" input is not
  // a compare, in which case it is further lowered.
  int UnsliceStart = SVA.getAsUnslice();
  if (UnsliceStart >= 0) {
    auto InnerSI = cast<ShuffleVectorInst>(SI->getOperand(1));
    auto WPR = Region::createWrPredRegion(SI->getOperand(0),
        InnerSI->getOperand(0), UnsliceStart, "", SI, SI->getDebugLoc());
    WPR->takeName(SI);
    SI->replaceAllUsesWith(WPR);
    // Undef out the operand for InnerSI in SI, so we can directly erase InnerSI
    // if SI was its only use.
    SI->setOperand(1, UndefValue::get(InnerSI->getType()));
    ToErase.push_back(SI);
    if (InnerSI->use_empty())
      InnerSI->eraseFromParent();
    return true;
  }
  // No other cases handled.
  SI->getContext().emitError(
      SI, "general bool shuffle vector instruction not implemented");
  return false;
}

/***********************************************************************
 * lowerBoolSplat : lower a shufflevector (element type i1) that is a splat
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 */
bool GenXLowering::lowerBoolSplat(ShuffleVectorInst *SI, Value *In, unsigned Idx)
{
  unsigned Width = SI->getType()->getVectorNumElements();
  if (isa<VectorType>(In->getType())) {
    IRBuilder<> B(SI);
    Constant *C1 = ConstantVector::getSplat(Width, B.getInt16(1));
    Constant *C0 = ConstantVector::getSplat(Width, B.getInt16(0));
    Value *V = B.CreateSelect(In, C1, C0);
    Region R(V);
    R.NumElements = Width;
    R.Stride = 0;
    R.VStride = 0;
    R.Offset = (int)Idx;
    V = R.createRdRegion(V, "splat", SI, SI->getDebugLoc());
    V = B.CreateICmpNE(V, C0);
    SI->replaceAllUsesWith(V);
    ToErase.push_back(SI);
    return true;
  }
  // This is a splat. See if the input is a cmp, possibly via a bitcast.
  if (auto BC = dyn_cast<BitCastInst>(In))
    In = BC->getOperand(0);
  if (auto Cmp = dyn_cast<CmpInst>(In)) {
    // Create a splatted version of the cmp.
    Value *CmpOpnds[2];
    Region R(Cmp->getOperand(0));
    R.NumElements = Width;
    R.Width = R.NumElements;
    R.Stride = 0;
    R.VStride = 0;
    for (unsigned i = 0; i != 2; ++i) {
      auto Opnd = Cmp->getOperand(i);
      if (auto C = dyn_cast<Constant>(Opnd)) {
        CmpOpnds[i] = ConstantVector::getSplat(R.NumElements, C);
        continue;
      }
      if (!isa<VectorType>(Opnd->getType())) {
        auto NewBC = CastInst::Create(Instruction::BitCast, Opnd,
            VectorType::get(Opnd->getType(), 1), Opnd->getName() + ".bc", Cmp);
        NewBC->setDebugLoc(Cmp->getDebugLoc());
        Opnd = NewBC;
      }
      CmpOpnds[i] = R.createRdRegion(Opnd,
          Cmp->getOperand(i)->getName() + ".splat", Cmp/*InsertBefore*/,
          Cmp->getDebugLoc());
    }
    auto NewCmp = CmpInst::Create(Cmp->getOpcode(), Cmp->getPredicate(),
        CmpOpnds[0], CmpOpnds[1], Cmp->getName() + ".splat",
        Cmp/*InsertBefore*/);
    NewCmp->setDebugLoc(Cmp->getDebugLoc());
    SI->replaceAllUsesWith(NewCmp);
    ToErase.push_back(SI);
    return true;
  }
  // Default code. Select int and bitcast to vector of i1.
  if (isa<VectorType>(In->getType())) {
    // First convert v1i1 to i1.
    auto NewBC = CastInst::Create(Instruction::BitCast, In,
        In->getType()->getScalarType(), In->getName() + ".scalar", SI);
    NewBC->setDebugLoc(SI->getDebugLoc());
    In = NewBC;
  }
  if (Width == 8 || Width == 16 || Width == 32) {
    auto IntTy = Type::getIntNTy(SI->getContext(), Width);
    auto Sel = SelectInst::Create(In, Constant::getAllOnesValue(IntTy),
                                  Constant::getNullValue(IntTy),
                                  SI->getName() + ".sel", SI);
    Sel->setDebugLoc(SI->getDebugLoc());
    auto NewBC = CastInst::Create(Instruction::BitCast, Sel, SI->getType(), "", SI);
    NewBC->takeName(SI);
    NewBC->setDebugLoc(SI->getDebugLoc());
    SI->replaceAllUsesWith(NewBC);
    ToErase.push_back(SI);
    return true;
  }

  IRBuilder<> Builder(SI);
  auto Val = Builder.CreateSelect(In, Builder.getInt16(1), Builder.getInt16(0),
                                  SI->getName() + ".sel");
  if (auto Inst = dyn_cast<Instruction>(Val))
    Inst->setDebugLoc(SI->getDebugLoc());
  Val = Builder.CreateBitCast(Val, VectorType::get(Builder.getInt16Ty(), 1));
  if (auto Inst = dyn_cast<Instruction>(Val))
    Inst->setDebugLoc(SI->getDebugLoc());

  Region R(Val);
  R.Offset = 0;
  R.Width = 1;
  R.Stride = R.VStride = 0;
  R.NumElements = Width;
  Val = R.createRdRegion(Val, "", SI, SI->getDebugLoc());
  Val = Builder.CreateICmpNE(Val, ConstantVector::getNullValue(Val->getType()));
  Val->takeName(SI);
  if (auto Inst = dyn_cast<Instruction>(Val))
    Inst->setDebugLoc(SI->getDebugLoc());
  SI->replaceAllUsesWith(Val);
  ToErase.push_back(SI);
  return true;
}

/***********************************************************************
* lowerShuffle : lower a ShuffleInst (element type not i1)
*
* Mostly these are splats. These are lowered to a rdregion
* Any other shuffle is currently unsupported
*/
bool GenXLowering::lowerShuffle(ShuffleVectorInst *SI)
{
  auto Splat = ShuffleVectorAnalyzer(SI).getAsSplat();
  if (Splat.Input) {
    // This is a splat. Turn it into a splatting rdregion.
    if (!isa<VectorType>(Splat.Input->getType())) {
      // The input is a scalar rather than a 1-vector. Bitcast it to a 1-vector.
      auto *BC = CastInst::Create(Instruction::BitCast, Splat.Input,
        VectorType::get(Splat.Input->getType(), 1), SI->getName(), SI);
      BC->setDebugLoc(SI->getDebugLoc());
      Splat.Input = BC;
    }
    // Create a rdregion with a stride of 0 to represent this splat
    Region R(Splat.Input);
    R.NumElements = SI->getType()->getVectorNumElements();
    R.Width = R.NumElements;
    R.Stride = 0;
    R.VStride = 0;
    R.Offset = Splat.Index * R.ElementBytes;
    Instruction *NewInst = R.createRdRegion(Splat.Input, "", SI/*InsertBefore*/,
      SI->getDebugLoc());
    NewInst->takeName(SI);
    NewInst->setDebugLoc(SI->getDebugLoc());
    SI->replaceAllUsesWith(NewInst);
    ToErase.push_back(SI);
    return true;
  }
  if (lowerShuffleToSelect(SI))
    return true;

  Value *Seralize = ShuffleVectorAnalyzer(SI).serialize();
  if (Seralize != nullptr) {
    SI->replaceAllUsesWith(Seralize);
    ToErase.push_back(SI);
    return true;
  }

  SI->getContext().emitError(
      SI, "generic shuffle-vector is not supported yet in GenX backend");
  return false;
}

// Lower those shufflevector that can be implemented efficiently as select.
bool GenXLowering::lowerShuffleToSelect(ShuffleVectorInst *SI) {
  int NumElements = SI->getType()->getVectorNumElements();
  int NumOpnd = SI->getNumOperands();
  for (int i = 0; i < NumOpnd; ++i) {
    if (SI->getOperand(i)->getType()->getVectorNumElements() != NumElements)
      return false;
  }
  for (int i = 0; i < NumElements; ++i) {
    int idx = SI->getMaskValue(i);
    // undef index returns -1.
    if (idx < 0)
      continue;
    if (idx != i && idx != i + NumElements)
      return false;
  }
  IRBuilder<> Builder(SI);
  Type *Int1Ty = Builder.getInt1Ty();
  SmallVector<Constant *, 16> MaskVec;
  MaskVec.reserve(NumElements);
  for (int i = 0; i < NumElements; ++i) {
    int idx = SI->getMaskValue(i);
    // undef index returns -1.
    if (idx == i || idx < 0)
      MaskVec.push_back(ConstantInt::get(Int1Ty, 1));
    else
      MaskVec.push_back(ConstantInt::get(Int1Ty, 0));
  }
  Value *Mask = ConstantVector::get(MaskVec);
  auto NewSel = SelectInst::Create(Mask, SI->getOperand(0),
                                   SI->getOperand(1), "", SI);
  NewSel->takeName(SI);
  NewSel->setDebugLoc(SI->getDebugLoc());
  SI->replaceAllUsesWith(NewSel);
  ToErase.push_back(SI);
  return true;
}

/***********************************************************************
 * lowerShr : lower Shl followed by AShr/LShr by the same amount
 *    into trunc+sext/zext
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * See convertShlShr below.
 */
bool GenXLowering::lowerShr(Instruction *Inst)
{
  Instruction *NewInst = convertShlShr(Inst);
  if (!NewInst)
    return false; // no conversion done
  ToErase.push_back(Inst);
  auto Shl = cast<Instruction>(Inst->getOperand(0));
  if (Shl->hasOneUse())
    ToErase.push_back(Shl);
  return true;
}

/***********************************************************************
 * convertShlShr : convert Shl followed by AShr/LShr by the same amount
 *    into trunc+sext/zext
 *
 * Enter:   Inst = the AShr or LShr instruction
 *
 * Return:  0 if no conversion done, else the new SExt/ZExt instruction.
 *          The original AShr/LShr is now unused, but neither original
 *          instruction is erased.
 *
 * This is the opposite to what instruction combining does! We want to change
 * it back to trunc then extend because the trunc can then be lowered into
 * a region, and the extend can sometimes be baled into whatever uses it.
 *
 * This is a separately callable global function so it can also be used
 * from GenXReduceIntSize, which for other reasons of convenience runs before
 * GenXLowering.
 */
Instruction *llvm::genx::convertShlShr(Instruction *Inst)
{
  unsigned NumBits = Inst->getType()->getScalarType()->getPrimitiveSizeInBits();
  auto C = dyn_cast<Constant>(Inst->getOperand(1));
  if (!C)
    return nullptr;
  auto Shl = dyn_cast<Instruction>(Inst->getOperand(0));
  if (!Shl)
    return nullptr;
  if (Shl->getOpcode() != Instruction::Shl)
    return nullptr;
  if (Shl->getOperand(1) != C)
    return nullptr;
  if (isa<VectorType>(C->getType())) {
    C = C->getSplatValue();
    if (!C)
      return nullptr;
  }
  unsigned ShiftBits = cast<ConstantInt>(C)->getSExtValue();
  unsigned RemainingBits = NumBits - ShiftBits;
  if (RemainingBits != 8 && RemainingBits != 16)
    return nullptr;
  // We have Shl+AShr or Shl+LShr that can be turned into trunc+sext/zext.
  Type *ConvTy = Type::getIntNTy(Inst->getContext(), RemainingBits);
  if (auto VT = dyn_cast<VectorType>(Inst->getType()))
    ConvTy = VectorType::get(ConvTy, VT->getNumElements());
  auto Trunc = CastInst::Create(Instruction::Trunc, Shl->getOperand(0),
      ConvTy, "", Inst);
  Trunc->takeName(Shl);
  Trunc->setDebugLoc(Shl->getDebugLoc());
  auto Ext = CastInst::Create(Inst->getOpcode() == Instruction::AShr
        ? Instruction::SExt : Instruction::ZExt,
      Trunc, Inst->getType(), "", Inst);
  Ext->takeName(Inst);
  Ext->setDebugLoc(Inst->getDebugLoc());
  Inst->replaceAllUsesWith(Ext);
  return Ext;
}

/***********************************************************************
 * splitStructPhis : find struct phi nodes and split them
 *
 * Return:  whether code modified
 *
 * Each struct phi node is split into a separate phi node for each struct
 * element. This is needed because the GenX backend's liveness and coalescing
 * code cannot cope with a struct phi.
 *
 * This is run in two places: firstly in GenXLowering, so that pass can then
 * simplify any InsertElement and ExtractElement instructions added by the
 * struct phi splitting. But then it needs to be run again in GenXLiveness,
 * because other passes can re-insert a struct phi. The case I saw in
 * hevc_speed was something commoning up the struct return from two calls in an
 * if..else..endif.
 */
bool genx::splitStructPhis(Function *F)
{
  bool Modified = false;
  for (Function::iterator fi = F->begin(), fe = F->end(); fi != fe; ++fi) {
    BasicBlock *BB = &*fi;
    for (BasicBlock::iterator bi = BB->begin(); ;) {
      PHINode *Phi = dyn_cast<PHINode>(&*bi);
      if (!Phi)
        break;
      ++bi; // increment here as splitStructPhi removes old phi node
      if (isa<StructType>(Phi->getType()))
        Modified |= GenXLowering::splitStructPhi(Phi);
    }
  }
  return Modified;
}

/***********************************************************************
 * splitStructPhi : split a phi node with struct type by splitting into
 *                  struct elements
 */
bool GenXLowering::splitStructPhi(PHINode *Phi)
{
  StructType *Ty = cast<StructType>(Phi->getType());
  // Find where we need to insert the combine instructions.
  Instruction *CombineInsertBefore = Phi->getParent()->getFirstNonPHI();
  // Now split the phi.
  Value *Combined = UndefValue::get(Ty);
  // For each struct element...
  for (unsigned Idx = 0, e = Ty->getNumElements(); Idx != e; ++Idx) {
    Type *ElTy = Ty->getTypeAtIndex(Idx);
    // Create the new phi node.
    PHINode *NewPhi = PHINode::Create(ElTy, Phi->getNumIncomingValues(),
        Phi->getName() + ".element" + Twine(Idx), Phi);
    NewPhi->setDebugLoc(Phi->getDebugLoc());
    // Combine the new phi.
    Instruction *Combine = InsertValueInst::Create(Combined, NewPhi,
        Idx, NewPhi->getName(), CombineInsertBefore);
    Combine->setDebugLoc(Phi->getDebugLoc());
    Combined = Combine;
    // For each incoming...
    for (unsigned In = 0, InEnd = Phi->getNumIncomingValues();
        In != InEnd; ++In) {
      // Create an extractelement to get the individual element value.
      // This needs to go before the terminator of the incoming block.
      BasicBlock *IncomingBB = Phi->getIncomingBlock(In);
      Value *Incoming = Phi->getIncomingValue(In);
      Instruction *Extract = ExtractValueInst::Create(Incoming, Idx,
          Phi->getName() + ".element" + Twine(Idx),
          IncomingBB->getTerminator());
      Extract->setDebugLoc(Phi->getDebugLoc());
      // Add as an incoming of the new phi node.
      NewPhi->addIncoming(Extract, IncomingBB);
    }
  }
  Phi->replaceAllUsesWith(Combined);
  Phi->eraseFromParent();
  return true;
}

/***********************************************************************
 * lowerExtractValue : remove extractvalue if possible
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * If we can trace the input of the extractvalue to the point where the
 * value was inserted, use that value instead.
 *
 * Because we have already split struct phi nodes, we should just be left
 * with insertvalue/extractvalue pairs that we can remove here. The
 * exception is when a struct is passed in to or returned from a call.
 * Then we leave the extractvalue for later handling in the register
 * allocator.
 */
bool GenXLowering::lowerExtractValue(ExtractValueInst *Inst)
{
  ArrayRef<unsigned> EVIndices = Inst->getIndices();
  ArrayRef<unsigned> Indices = EVIndices;
  Value *V = Inst->getAggregateOperand();
  for (;;) {
    InsertValueInst *IV = dyn_cast<InsertValueInst>(V);
    if (!IV) {
      // If we used up any indices, create a new extractvalue for the
      // remaining ones.
      if (Indices.size() != EVIndices.size()) {
        Instruction *NewIV = ExtractValueInst::Create(
            Inst->getAggregateOperand(), Indices, Inst->getName(), Inst);
        NewIV->setDebugLoc(Inst->getDebugLoc());
        Inst->replaceAllUsesWith(NewIV);
        ToErase.push_back(Inst);
        return true;
      }
      return false;
    }
    // We have an insertvalue. See how many of the indices agree.
    ArrayRef<unsigned> IVIndices = IV->getIndices();
    unsigned Match = 0;
    while (Match < Indices.size() && Match < IVIndices.size()
        && Indices[Match] == IVIndices[Match])
      ++Match;
    if (!Match) {
      // No match at all. Go back to the previous insertvalue.
      V = IV->getAggregateOperand();
      continue;
    }
    // Use the inserted value here.
    V = IV->getInsertedValueOperand();
    // Chop off the indices we have used up. If none left, we have finished.
    Indices = Indices.slice(Match);
    if (!Indices.size())
      break;
  }
  // We have found the struct element value V.
  Inst->replaceAllUsesWith(V);
  ToErase.push_back(Inst);
  return true;
}

/***********************************************************************
 * lowerInsertValue : remove insertvalue if possible
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * In most cases, by the time we get to an insertvalue, it will be unused
 * because of extractvalue removal.
 *
 * In a case where it is still used (probably because this function has an
 * arg or return value that is a struct, or we call a function like that),
 * the struct value is dealt with in register allocation.
 */
bool GenXLowering::lowerInsertValue(InsertValueInst *Inst)
{
  if (Inst->use_empty()) {
    ToErase.push_back(Inst);
    return true;
  }
  return false;
}

/***********************************************************************
 * lowerUAddWithOverflow : lower llvm.uadd.with.overflow
 *
 * This could potentially be implemented with the vISA addc instruction.
 * However an intrinsic for that would need extra GenX backend support for
 * returning a struct containing two vectors, and that support does not exist
 * now.
 *
 * So for now we use the old DEC Alpha trick of comparing the result with
 * one of the operands.
 */
bool GenXLowering::lowerUAddWithOverflow(CallInst *CI)
{
  DebugLoc DL = CI->getDebugLoc();
  // Do the add.
  auto Add = BinaryOperator::Create(Instruction::Add, CI->getArgOperand(0),
      CI->getArgOperand(1), CI->getName() + ".add", CI);
  Add->setDebugLoc(DL);
  // Do the comparison. (An unsigned add has overflowed if the result is
  // smaller than one of the operands, and, if it has overflowed, the result
  // is smaller than both of the operands. So it doesn't matter which operand
  // we use for the comparison.)
  auto Cmp = CmpInst::Create(Instruction::ICmp, CmpInst::ICMP_ULT, Add,
      CI->getArgOperand(1), CI->getName() + ".cmp", CI);
  Cmp->setDebugLoc(DL);
  // For any extractvalue use of the result of the original add with overflow,
  // replace it directly.
  SmallVector<ExtractValueInst *, 4> Extracts;
  for (auto ui = CI->use_begin(), ue = CI->use_end(); ui != ue; ++ui)
    if (auto EVI = dyn_cast<ExtractValueInst>(ui->getUser()))
      Extracts.push_back(EVI);
  for (auto ei = Extracts.begin(), ee = Extracts.end(); ei != ee; ++ei) {
    auto EVI = *ei;
    EVI->replaceAllUsesWith(EVI->getIndices()[0] ? (Value *)Cmp : (Value *)Add);
    EVI->setOperand(0, UndefValue::get(CI->getType()));
    ToErase.push_back(EVI);
  }
  // If any uses of the original intrinsic remain, recreate the struct value.
  if (!CI->use_empty()) {
    auto Insert = InsertValueInst::Create(UndefValue::get(CI->getType()), Add, 0,
        CI->getName() + ".insertadd", CI);
    Insert->setDebugLoc(DL);
    Insert = InsertValueInst::Create(Insert, Cmp, 1,
        CI->getName() + ".insertcmp", CI);
    Insert->setDebugLoc(DL);
    // ... and use it to replace the original intrinsic.
    CI->replaceAllUsesWith(Insert);
  }
  ToErase.push_back(CI);
  return true;
}

// Lower cmp instructions that GenX cannot deal with.
bool GenXLowering::lowerFCmpInst(FCmpInst *Inst) {
  IRBuilder<> Builder(Inst);
  Builder.SetCurrentDebugLocation(Inst->getDebugLoc());
  Value *Ops[] = {Inst->getOperand(0), Inst->getOperand(1)};

  switch (Inst->getPredicate()) {
  default:
    break;
  case CmpInst::FCMP_ORD: // True if ordered (no nans)
  {
    // %c = fcmp ord %a %b
    // =>
    // %1 = fcmp oeq %a %a
    // %2 = fcmp oeq %b %b
    // %c = and %1 %2
    Value *LHS = Builder.CreateFCmpOEQ(Ops[0], Ops[0]);
    Value *RHS = Builder.CreateFCmpOEQ(Ops[1], Ops[1]);
    Value *New = Builder.CreateAnd(LHS, RHS);
    Inst->replaceAllUsesWith(New);
    ToErase.push_back(Inst);
    return true;
  }
  case CmpInst::FCMP_UNO: // True if unordered: isnan(X) | isnan(Y)
    // %c = fcmp uno %a %b
    // =>
    // %1 = fcmp une %a %a
    // %2 = fcmp une %b %b
    // %c = or %1 %2
    Value *LHS = Builder.CreateFCmpUNE(Ops[0], Ops[0]);
    Value *RHS = Builder.CreateFCmpUNE(Ops[1], Ops[1]);
    Value *New = Builder.CreateOr(LHS, RHS);
    Inst->replaceAllUsesWith(New);
    ToErase.push_back(Inst);
    return true;
  }

  return false;
}

// Lower cmp instructions that GenX cannot deal with.
bool GenXLowering::lowerMul64(Instruction *Inst) {
  IRBuilder<> Builder(Inst);
  Builder.SetCurrentDebugLocation(Inst->getDebugLoc());
  auto Src0 = Inst->getOperand(0);
  auto Src1 = Inst->getOperand(1);
  auto ETy = Src0->getType();
  auto Len = 1;
  if (ETy->isVectorTy()) {
    Len = ETy->getVectorNumElements();
    ETy = ETy->getVectorElementType();
  }
  if (!ETy->isIntegerTy() || ETy->getPrimitiveSizeInBits() != 64)
    return false;
  auto VTy = VectorType::get(ETy->getInt32Ty(Inst->getContext()), Len * 2);
  // create src0 bitcast, then the low and high part
  auto Src0V = Builder.CreateBitCast(Src0, VTy);
  Region R(Inst);
  R.Offset = 0;
  R.Width = Len;  R.NumElements = Len;
  R.Stride = 2;  R.VStride = 0;
  auto Src0L = R.createRdRegion(Src0V, "", Inst, Inst->getDebugLoc());
  R.Offset = 4;
  auto Src0H = R.createRdRegion(Src0V, "", Inst, Inst->getDebugLoc());
  // create src1 bitcast, then the low and high part
  auto Src1V = Builder.CreateBitCast(Src1, VTy);
  R.Offset = 0;
  auto Src1L = R.createRdRegion(Src1V, "", Inst, Inst->getDebugLoc());
  R.Offset = 4;
  auto Src1H = R.createRdRegion(Src1V, "", Inst, Inst->getDebugLoc());
  // create muls and adds
  auto ResL = Builder.CreateMul(Src0L, Src1L);
  // create the mulh intrinsic to the get the carry-part
  Type *tys[2];
  SmallVector<llvm::Value*, 2> args;
  // build type-list
  tys[0] = ResL->getType();
  tys[1] = Src0L->getType();
  // build argument list
  args.push_back(Src0L);
  args.push_back(Src1L);
  auto M = Inst->getParent()->getParent()->getParent();
  Function *IntrinFunc = Intrinsic::getDeclaration(M, Intrinsic::genx_umulh, tys);
  Instruction *Cari = CallInst::Create(IntrinFunc, args, "", Inst);
  Cari->setDebugLoc(Inst->getDebugLoc());
  auto Temp0 = Builder.CreateMul(Src0L, Src1H);
  auto Temp1 = Builder.CreateAdd(Cari, Temp0);
  auto Temp2 = Builder.CreateMul(Src0H, Src1L);
  auto ResH = Builder.CreateAdd(Temp2, Temp1);
  // create the write-regions
  auto UndefV = UndefValue::get(VTy);
  R.Offset = 0;
  auto WrL = R.createWrRegion(UndefV, ResL, "WrLow", Inst, Inst->getDebugLoc());
  R.Offset = 4;
  auto WrH = R.createWrRegion(WrL, ResH, "WrHigh", Inst, Inst->getDebugLoc());
  // create the bitcast to the destination-type
  auto Replace = Builder.CreateBitCast(WrH, Inst->getType(), "mul64");
  Inst->replaceAllUsesWith(Replace);
  ToErase.push_back(Inst);
  return true;
}
/***********************************************************************
 * widenByteOp : widen a vector byte operation to short if that might
 *               improve code
 *
 * Return:  whether any change was made, and thus the current instruction
 *          is now marked for erasing
 *
 * Gen has restrictions on byte operands. The jitter copes with that, but
 * sometimes it needs to do even-odd splitting, which can lead to suboptimal
 * code if cmps and predicates are involved.
 * Here we attempt to pick up the common cases by converting a byte operation
 * to short.
 *
 * Note that we might end up with the extends being baled into the instruction
 * anyway, resulting in a byte operation in vISA.
 */
bool GenXLowering::widenByteOp(Instruction *Inst)
{
  if (!EnableGenXByteWidening)
    return false;
  Type *Ty = Inst->getType();
  if (isa<CmpInst>(Inst))
    Ty = Inst->getOperand(0)->getType();
  if (!isa<VectorType>(Ty) || !Ty->getScalarType()->isIntegerTy(8))
    return false; // not byte operation
  if (Inst->use_empty())
    return false; // result unused
  // check use, if use is a phi, stop widenning
  if (!isa<CmpInst>(Inst)) {
    for (auto ui = Inst->use_begin(), ue = Inst->use_end(); ui != ue; ++ui) {
      auto User = cast<Instruction>(ui->getUser());
      if (isa<PHINode>(User))
        return false;
    }
  }
  // For a predicated wrregion, widen by separating the predication into a
  // rdregion and select, which can then be widened.
  if (isWrRegion(getIntrinsicID(Inst))) {
    Region R(Inst, BaleInfo());
    if (R.NumElements == 1 || !R.Mask)
      return false;
    // Can only do this if the predicate is the right size. (We could handle
    // the wrong size case by adding an rdpredregion, but then we would need
    // to ensure that GenXLegalization can cope with an arbitrary size
    // rdpredregion.)
    if (R.Mask->getType()->getVectorNumElements() != R.NumElements)
      return false;
    // Create the rdregion and select.
    auto NewRd = R.createRdRegion(Inst->getOperand(0),
        Inst->getName() + ".byteselrdr", Inst, Inst->getDebugLoc());
    auto NewSel = SelectInst::Create(R.Mask, Inst->getOperand(1), NewRd,
        "", Inst);
    NewSel->takeName(Inst);
    NewSel->setDebugLoc(Inst->getDebugLoc());
    // Modify the existing wrregion.
    Inst->setName(NewSel->getName() + ".byteselwrr");
    Inst->setOperand(1, NewSel);
    Inst->setOperand(Intrinsic::GenXRegion::PredicateOperandNum,
        Constant::getAllOnesValue(R.Mask->getType()));
    // Fall through for the select to get widened.
    Inst = NewSel;
  }
  // Do the widening for:
  // 1. a compare or select
  // 2. used in a zext that indicates that the user has probably already been
  //    widened by this code.
  bool Widen = false;
  if (isa<CmpInst>(Inst) || isa<SelectInst>(Inst))
    Widen = true;
  else {
    auto user = cast<Instruction>(Inst->use_begin()->getUser());
    if (isa<ZExtInst>(user))
      Widen = true;
  }
  if (!Widen)
    return false;
  // Widen to short.
  // Decide whether to zero or sign extend. Also decide whether the result is
  // guaranteed to have all 0 bits in the extended part.
  Instruction::CastOps ExtOpcode = Instruction::ZExt;
  bool ExtendedIsZero = false;
  switch (Inst->getOpcode()) {
    case Instruction::SDiv:
    case Instruction::AShr:
      ExtOpcode = Instruction::SExt;
      break;
    case Instruction::And:
    case Instruction::Or:
    case Instruction::Xor:
    case Instruction::LShr:
      ExtendedIsZero = true;
      break;
    case Instruction::ICmp:
      if (cast<CmpInst>(Inst)->isSigned())
        ExtOpcode = Instruction::SExt;
      break;
    default:
      break;
  }
  // Get the range of operands to process.
  unsigned StartIdx = 0, EndIdx = Inst->getNumOperands();
  if (auto CI = dyn_cast<CallInst>(Inst))
    EndIdx = CI->getNumArgOperands();
  else if (isa<SelectInst>(Inst))
    StartIdx = 1;
  // Extend the operands.
  auto ExtTy = VectorType::get(Type::getInt16Ty(Inst->getContext()),
      Inst->getOperand(StartIdx)->getType()->getVectorNumElements());
  SmallVector<Value *, 4> Opnds;
  for (unsigned Idx = 0; Idx != EndIdx; ++Idx) {
    Value *Opnd = Inst->getOperand(Idx);
    if (Idx >= StartIdx) {
      if (auto C = dyn_cast<Constant>(Opnd))
        Opnd = ConstantExpr::getCast(ExtOpcode, C, ExtTy);
      else {
        auto NewExt = CastInst::Create(ExtOpcode, Opnd, ExtTy,
            Inst->getName() + ".byteext", Inst);
        NewExt->setDebugLoc(Inst->getDebugLoc());
        Opnd = NewExt;
      }
    }
    Opnds.push_back(Opnd);
  }
  // Create the replacement instruction.
  Instruction *NewInst = nullptr;
  if (isa<BinaryOperator>(Inst))
    NewInst = BinaryOperator::Create((Instruction::BinaryOps)Inst->getOpcode(),
        Opnds[0], Opnds[1], "", Inst);
  else if (auto CI = dyn_cast<CmpInst>(Inst))
    NewInst = CmpInst::Create(CI->getOpcode(), CI->getPredicate(),
        Opnds[0], Opnds[1], "", CI);
  else if (isa<SelectInst>(Inst))
    NewInst = SelectInst::Create(Opnds[0], Opnds[1], Opnds[2], "", Inst);
  else
    llvm_unreachable("unhandled instruction in widenByteOp");
  NewInst->takeName(Inst);
  NewInst->setDebugLoc(Inst->getDebugLoc());
  if (ExtendedIsZero) {
    // We know that the extended part of the result contains 0 bits. If we
    // find that any use is a zext (probably from also being byte widened
    // in this code), we can replace the use directly and save the
    // trunc/zext pair. First put the uses in a vector as the use list will
    // change under our feet.
    SmallVector<Use *, 4> Uses;
    for (auto ui = Inst->use_begin(), ue = Inst->use_end(); ui != ue; ++ui)
      Uses.push_back(&*ui);
    for (auto ui = Uses.begin(), ue = Uses.end(); ui != ue; ++ui) {
      if (auto user = dyn_cast<ZExtInst>((*ui)->getUser())) {
        if (user->getType() == NewInst->getType()) {
          user->replaceAllUsesWith(NewInst);
          ToErase.push_back(user);
          // Remove the use of Inst from the trunc so we can tell whether there
          // are any uses left below.
          *(*ui) = UndefValue::get(Inst->getType());
        }
      }
    }
  }
  if (!Inst->use_empty()) {
    // Truncate the result.
    if (!isa<CmpInst>(Inst)) {
      NewInst = CastInst::Create(Instruction::Trunc, NewInst, Inst->getType(),
          Inst->getName() + ".bytetrunc", Inst);
      NewInst->setDebugLoc(Inst->getDebugLoc());
    }
    // Replace uses.
    Inst->replaceAllUsesWith(NewInst);
  }
  ToErase.push_back(Inst);
  return true;
}

static bool breakConstantVector(unsigned i, Instruction *CurInst,
                                Instruction *InsertPt) {
  ConstantVector *CV = cast<ConstantVector>(CurInst->getOperand(i));

  // Splat case.
  if (auto S = dyn_cast_or_null<ConstantExpr>(CV->getSplatValue())) {
    // Turn element into an instruction
    auto Inst = S->getAsInstruction();
    Inst->setDebugLoc(CurInst->getDebugLoc());
    Inst->insertBefore(InsertPt);
    Type *NewTy = VectorType::get(Inst->getType(), 1);
    Inst = CastInst::Create(Instruction::CastOps::BitCast, Inst, NewTy, "",
                            CurInst);
    Inst->setDebugLoc(CurInst->getDebugLoc());

    // Splat this value.
    Region R(Inst);
    R.Offset = 0;
    R.Width = 1;
    R.Stride = R.VStride = 0;
    R.NumElements = CV->getNumOperands();
    Inst = R.createRdRegion(Inst, "", InsertPt /*InsertBefore*/,
                            Inst->getDebugLoc());

    // Update i-th operand with newly created splat.
    CurInst->setOperand(i, Inst);
    return true;
  }

  SmallVector<Value *, 8> Vals;
  bool HasConstExpr = false;
  for (unsigned j = 0, N = CV->getNumOperands(); j < N; ++j) {
    Value *Elt = CV->getOperand(j);
    if (auto CE = dyn_cast<ConstantExpr>(Elt)) {
      auto Inst = CE->getAsInstruction();
      Inst->setDebugLoc(CurInst->getDebugLoc());
      Inst->insertBefore(InsertPt);
      Vals.push_back(Inst);
      HasConstExpr = true;
    } else
      Vals.push_back(Elt);
  }

  if (HasConstExpr) {
    Value *Val = UndefValue::get(CV->getType());
    for (unsigned j = 0, N = CV->getNumOperands(); j < N; ++j) {
      Region R(Vals[j]);
      R.Offset = j * R.ElementBytes;
      Val = R.createWrRegion(Val, Vals[j], "", InsertPt, CurInst->getDebugLoc());
    }
    CurInst->setOperand(i, Val);
    return true;
  }

  return false;
}

bool genx::breakConstantExprs(Function *F) {
  bool Modified = false;
  for (po_iterator<BasicBlock *> i = po_begin(&F->getEntryBlock()),
                                 e = po_end(&F->getEntryBlock());
       i != e; ++i) {
    BasicBlock *BB = *i;
    // The effect of this loop is that we process the instructions in reverse
    // order, and we re-process anything inserted before the instruction
    // being processed.
    for (Instruction *CurInst = BB->getTerminator(); CurInst;) {
      PHINode *PN = dyn_cast<PHINode>(CurInst);
      for (unsigned i = 0, e = CurInst->getNumOperands(); i < e; ++i) {
        Instruction *InsertPt =
            PN ? PN->getIncomingBlock(i)->getTerminator() : CurInst;
        Value *Op = CurInst->getOperand(i);
        if (getUnderlyingGlobalVariable(Op) != nullptr)
          continue;
        if (ConstantExpr *CE = dyn_cast<ConstantExpr>(Op)) {
          Instruction *NewInst = CE->getAsInstruction();
          NewInst->setDebugLoc(CurInst->getDebugLoc());
          NewInst->insertBefore(CurInst);
          CurInst->setOperand(i, NewInst);
          Modified = true;
        } else if (isa<ConstantVector>(Op))
          Modified |= breakConstantVector(i, CurInst, InsertPt);
      }
      CurInst = CurInst == &BB->front() ? nullptr : CurInst->getPrevNode();
    }
  }
  return Modified;
}

namespace {

// Helper class to translate load/store into proper GenX intrinsic calls.
class LoadStoreResolver {
  Instruction *Inst;
  const GenXSubtarget *ST;
  IRBuilder<> Builder;

public:
  LoadStoreResolver(Instruction *Inst, const GenXSubtarget *ST)
      : Inst(Inst), ST(ST), Builder(Inst) {}

  // Resolve this instruction and return true on success.
  bool resolve();

private:
  bool isLoad() const { return isa<LoadInst>(Inst); }
  bool isStore() const { return isa<StoreInst>(Inst); }

  const DataLayout &getDL() const {
    Function *F = Inst->getParent()->getParent();
    return F->getParent()->getDataLayout();
  }

  // Find a proper GenX intrinsic ID for this load/store instruction.
  Intrinsic::ID getIntrinsicID() const;

  unsigned getPointerSizeInBits() const {
    unsigned AddrSp = 0;
    if (auto LI = dyn_cast<LoadInst>(Inst))
      AddrSp = LI->getPointerAddressSpace();
    else if (auto SI = dyn_cast<StoreInst>(Inst))
      AddrSp = SI->getPointerAddressSpace();
    return getDL().getPointerSizeInBits(AddrSp);
  }

  unsigned getValueSizeInBits(Type *T) const {
    if (auto PT = dyn_cast<PointerType>(T)) {
      unsigned AddrSp = PT->getAddressSpace();
      return getDL().getPointerSizeInBits(AddrSp);
    }
    return T->getPrimitiveSizeInBits();
  }

  // Return true if this load/store can be translated.
  bool isSupported() const;

  // Emit actual intrinsic calls.
  bool emitGather();
  bool emitScatter();
  bool emitSVMGather();
  bool emitSVMScatter();
};

} // namespace

// Translate store instructions into genx builtins.
bool GenXLowering::lowerLoadStore(Instruction *Inst) {
  auto ST = getAnalysisIfAvailable<GenXSubtargetPass>();
  LoadStoreResolver Resolver(Inst, ST ? ST->getSubtarget() : nullptr);
  if (Resolver.resolve()) {
    ToErase.push_back(Inst);
    return true;
  }
  return false;
}

bool LoadStoreResolver::resolve() {
  if (!isSupported())
    return false;

  Intrinsic::ID ID = getIntrinsicID();
  switch (ID) {
  case Intrinsic::genx_gather_scaled:
    return emitGather();
  case Intrinsic::genx_scatter_scaled:
    return emitScatter();
  case Intrinsic::genx_svm_gather:
    return emitSVMGather();
  case Intrinsic::genx_svm_scatter:
    return emitSVMScatter();
  default:
    break;
  }

  return false;
}

// Return true if this load/store can be translated.
bool LoadStoreResolver::isSupported() const {
  auto IsGlobalLoadStore = [=]() {
    Value *Ptr = nullptr;
    if (auto LI = dyn_cast<LoadInst>(Inst))
      Ptr = LI->getPointerOperand();
    if (auto SI = dyn_cast<StoreInst>(Inst))
      Ptr = SI->getPointerOperand();
    return getUnderlyingGlobalVariable(Ptr) != nullptr;
  };

  if (IsGlobalLoadStore())
    return false;

  Type *ValTy = Inst->getType();
  if (auto SI = dyn_cast<StoreInst>(Inst))
    ValTy = SI->getValueOperand()->getType();

  // Only scalar data types.
  if (!ValTy->isFloatingPointTy() && !ValTy->isIntegerTy() &&
      !ValTy->isPointerTy()) {
    Inst->getContext().emitError(Inst, "unsupported type for load/store");
    return false;
  }

  // Only legal types: float, double, half, i8, i16, 132, i64, pointer types.
  unsigned NumBits = getValueSizeInBits(ValTy);
  if (NumBits < 8 || NumBits > 64 || !isPowerOf2_32(NumBits)) {
    Inst->getContext().emitError("unsupported integer type for load/store");
    return false;
  }

  // Translate this instruction.
  return true;
}

// Find a proper GenX intrinsic ID for this load/store instruction.
Intrinsic::ID LoadStoreResolver::getIntrinsicID() const {
  // A32 byte scattered stateless messages only work on CNL+.
  unsigned NBits = getPointerSizeInBits();
  if (NBits == 32 && ST && !ST->WaNoA32ByteScatteredStatelessMessages())
    return isLoad() ? Intrinsic::genx_gather_scaled
                    : Intrinsic::genx_scatter_scaled;
  return isLoad() ? Intrinsic::genx_svm_gather : Intrinsic::genx_svm_scatter;
}

bool LoadStoreResolver::emitGather() {
  unsigned NBits = getPointerSizeInBits();
  Type *IntTy = IntegerType::get(Inst->getContext(), NBits);
  auto LI = cast<LoadInst>(Inst);

  // Global offset.
  Value *Addr = LI->getPointerOperand();
  Addr = Builder.CreatePtrToInt(Addr, IntTy);

  unsigned NBlocks = getValueSizeInBits(LI->getType()) / 8;
  unsigned NBlocksLog2 = llvm::Log2_32(NBlocks);

  // If this is more than 4 bytes, use a larger SIMD size.
  unsigned SIMD = 1;
  if (NBlocks > 4) {
    SIMD = NBlocks / 4;
    NBlocksLog2 = 2;
  }

  // The old value is undef.
  Type *ValTy = LI->getType();
  if (ValTy->isPointerTy())
    ValTy = Builder.getIntNTy(getValueSizeInBits(ValTy));
  Type *DataTy = VectorType::get(ValTy, 1);
  if (SIMD > 1)
    DataTy = VectorType::get(Builder.getInt32Ty(), SIMD);
  Value *OldVal = UndefValue::get(DataTy);

  // Offset.
  Type *EltOffsetTy = VectorType::get(Builder.getInt32Ty(), SIMD);
  Value *EltOffset = Constant::getNullValue(EltOffsetTy);
  if (SIMD > 1) {
     SmallVector<uint32_t, 2> Offsets(SIMD);
     for (unsigned i = 0; i < SIMD; ++i)
       // Increase offset by 4 bytes for each lane.
       Offsets[i] = i * 4;
     EltOffset = ConstantDataVector::get(Inst->getContext(), Offsets);
  }

  // Arguments.
  Value *Args[] = {
      Constant::getAllOnesValue(VectorType::get(Builder.getInt1Ty(), SIMD)),
      Builder.getInt32(NBlocksLog2),   // log[2](NBlocks)
      Builder.getInt16(0),             // scale
      Builder.getInt32(visa::getT5()), // surface
      Addr,                            // global offset
      EltOffset,                       // element offset
      OldVal                           // old value
  };

  // Overload with return type, predicate type and element offset type
  Type *Tys[] = { OldVal->getType(), Args[0]->getType(), EltOffsetTy };
  Module *M = Inst->getParent()->getParent()->getParent();
  auto Fn = Intrinsic::getDeclaration(M, Intrinsic::genx_gather_scaled, Tys);

  Value *NewVal = Builder.CreateCall(Fn, Args);
  NewVal = Builder.CreateBitCast(NewVal, ValTy);
  LI->replaceAllUsesWith(NewVal);
  return true;
}

bool LoadStoreResolver::emitScatter() {
  unsigned NBits = getPointerSizeInBits();
  Type *IntTy = IntegerType::get(Inst->getContext(), NBits);
  auto SI = cast<StoreInst>(Inst);

  // Global offset.
  Value *Addr = SI->getPointerOperand();
  Addr = Builder.CreatePtrToInt(Addr, IntTy);

  Value *Val = SI->getValueOperand();
  unsigned NBlocks = getValueSizeInBits(Val->getType()) / 8;
  unsigned NBlocksLog2 = llvm::Log2_32(NBlocks);

  // If this is more than 4 bytes, use a larger SIMD size.
  unsigned SIMD = 1;
  if (NBlocks > 4) {
    SIMD = NBlocks / 4;
    NBlocksLog2 = 2;
  }

  // Value to write.
  Type *ValTy = (SIMD > 1) ? Builder.getInt32Ty() : Val->getType();
  if (ValTy->isPointerTy())
    ValTy = Builder.getIntNTy(getValueSizeInBits(ValTy));
  Val = Builder.CreateBitCast(Val, VectorType::get(ValTy, SIMD));

  // Offset.
  Type *EltOffsetTy = VectorType::get(Builder.getInt32Ty(), SIMD);
  Value *EltOffset = Constant::getNullValue(EltOffsetTy);
  if (SIMD > 1) {
    SmallVector<uint32_t, 2> Offsets(SIMD);
    // Increase offset by 4 bytes for each lane.
    for (unsigned i = 0; i < SIMD; ++i)
      Offsets[i] = i * 4;
    EltOffset = ConstantDataVector::get(Inst->getContext(), Offsets);
  }

  // Arguments.
  Value *Args[] = {
      Constant::getAllOnesValue(VectorType::get(Builder.getInt1Ty(), SIMD)),
      Builder.getInt32(NBlocksLog2),   // log[2](NBlocks)
      Builder.getInt16(0),             // scale
      Builder.getInt32(visa::getT5()), // surface
      Addr,                            // global offset
      EltOffset,                       // element offset
      Val                              // value to write
  };

  // Overload with predicate type, element offset type, value to write type.
  Type *Tys[] = {Args[0]->getType(), EltOffsetTy, Val->getType()};
  Module *M = Inst->getParent()->getParent()->getParent();
  auto Fn = Intrinsic::getDeclaration(M, Intrinsic::genx_scatter_scaled, Tys);
  Builder.CreateCall(Fn, Args);
  return true;
}

// Compute the block size and the number of blocks for svm gather/scatter.
//
// Block_Size, 1, 4, 8
// Num_Blocks, 1, 2, 4,
//             8 only valid for 4 byte blocks and execution size 8.
//
static unsigned getBlockCount(Type *Ty) {
  unsigned NumBytes = Ty->getPrimitiveSizeInBits() / 8;
  assert(NumBytes <= 8 && "out of sync");

  // If this is N = 2 byte data, use 2 blocks;
  // otherwise, use 1 block of N bytes.
  return (NumBytes == 2) ? NumBytes : 1U;
}

// Translate store to svm scatter.
bool LoadStoreResolver::emitSVMGather() {
  unsigned NBits = getPointerSizeInBits();
  Type *IntTy = IntegerType::get(Inst->getContext(), NBits);
  auto LI = cast<LoadInst>(Inst);

  // Address.
  Value *Addr = LI->getPointerOperand();
  Addr = Builder.CreatePtrToInt(Addr, IntTy);
  if (NBits == 32)
    Addr = Builder.CreateZExt(Addr, Builder.getInt64Ty());
  Addr = Builder.CreateBitCast(Addr, VectorType::get(Addr->getType(), 1));

  // The old value is undef.
  Type *ValTy = LI->getType();
  if (ValTy->isPointerTy())
    ValTy = Builder.getIntNTy(getValueSizeInBits(ValTy));
  Type *DataTy = VectorType::get(ValTy, 1);
  Value *OldVal = UndefValue::get(DataTy);

  // Num of blocks.
  unsigned NBlocks = getBlockCount(OldVal->getType());
  unsigned NBlocksLog2 = llvm::Log2_32(NBlocks);

  Value *Args[] = {
      Constant::getAllOnesValue(VectorType::get(Builder.getInt1Ty(), 1)),
      Builder.getInt32(NBlocksLog2), // log2(num_of_blocks)
      Addr,                          // addresses
      OldVal                         // old value
  };

  // Overload with return type, predicate type and address vector type
  Type *Tys[] = {OldVal->getType(), Args[0]->getType(), Addr->getType()};
  Module *M = Inst->getParent()->getParent()->getParent();
  auto Fn = Intrinsic::getDeclaration(M, Intrinsic::genx_svm_gather, Tys);

  Value *NewVal = Builder.CreateCall(Fn, Args);
  NewVal = Builder.CreateBitCast(NewVal, ValTy);
  if (LI->getType()->isPointerTy())
    NewVal = Builder.CreateIntToPtr(NewVal, LI->getType());
  LI->replaceAllUsesWith(NewVal);
  return true;
}

bool LoadStoreResolver::emitSVMScatter() {
  unsigned NBits = getPointerSizeInBits();
  Type *IntTy = IntegerType::get(Inst->getContext(), NBits);
  auto SI = cast<StoreInst>(Inst);

  // Address
  Value *Addr = SI->getPointerOperand();
  Addr = Builder.CreatePtrToInt(Addr, IntTy);
  if (NBits == 32)
    Addr = Builder.CreateZExt(Addr, Builder.getInt64Ty());
  Addr = Builder.CreateBitCast(Addr, VectorType::get(Addr->getType(), 1));

  // data to write.
  Value *Val = SI->getValueOperand();
  Type *ValTy = Val->getType();
  if (ValTy->isPointerTy()) {
    ValTy = Builder.getIntNTy(getValueSizeInBits(ValTy));
    Val = Builder.CreatePtrToInt(Val, ValTy);
  }
  Val = Builder.CreateBitCast(Val, VectorType::get(ValTy, 1));

  // Num of blocks.
  unsigned NBlocks = getBlockCount(Val->getType());
  unsigned NBlocksLog2 = llvm::Log2_32(NBlocks);

  Value *Args[] = {
      Constant::getAllOnesValue(VectorType::get(Builder.getInt1Ty(), 1)),
      Builder.getInt32(NBlocksLog2), // log2(num_of_blocks)
      Addr,                          // addresses
      Val                            // value to write
  };

  // Overload with predicate type, address vector type, and data type
  Type *Tys[] = {Args[0]->getType(), Addr->getType(), Val->getType()};
  Module *M = Inst->getParent()->getParent()->getParent();
  auto Fn = Intrinsic::getDeclaration(M, Intrinsic::genx_svm_scatter, Tys);

  Builder.CreateCall(Fn, Args);
  return true;
}
